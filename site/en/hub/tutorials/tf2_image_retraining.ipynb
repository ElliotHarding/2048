{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScitaPqhKtuW"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvztxQ6VsK2k"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# Retraining an Image Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_image_retraining\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/hub/tutorials/tf2_image_retraining.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/collections/image/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otmJgmbahf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Image classification models have millions of parameters. Training them from\n",
        "scratch requires a lot of labeled training data and a lot of computing power. Transfer learning is a technique that shortcuts much of this by taking a piece of a model that has already been trained on a related task and reusing it in a new model.\n",
        "\n",
        "This Colab demonstrates how to build a Keras model for classifying five species of flowers by using a pre-trained TF2 SavedModel from TensorFlow Hub for image feature extraction, trained on the much larger and more general ImageNet dataset. Optionally, the feature extractor can be trained (\"fine-tuned\") alongside the newly added classifier.\n",
        "\n",
        "### Looking for a tool instead?\n",
        "\n",
        "This is a TensorFlow coding tutorial. If you want a tool that just builds the TensorFlow or TFLite model for, take a look at the [make_image_classifier](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) command-line tool that gets [installed](https://www.tensorflow.org/hub/installation) by the PIP package `tensorflow-hub[make_image_classifier]`, or at [this](https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb) TFLite colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dlauq-4FWGZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f750b334-9ef0-42b5-f5c8-1c96bf3905d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.18.0\n",
            "Hub version: 0.16.1\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Select the TF2 SavedModel module to use\n",
        "\n",
        "For starters, use [https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4). The same URL can be used in code to identify the SavedModel and in your browser to show its documentation. (Note that models in TF1 Hub format won't work here.)\n",
        "\n",
        "You can find more TF2 models that generate image feature vectors [here](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2).\n",
        "\n",
        "There are multiple possible models to try. All you need to do is select a different one on the cell below and follow up with the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FlsEcKVeuCnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3834b134-3902-4d37-e586-d64da64f84e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: efficientnetv2-xl-21k : https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\n",
            "Input size (512, 512)\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "model_name = \"efficientnetv2-xl-21k\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
        "\n",
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300,\n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "## Set up the Flowers dataset\n",
        "\n",
        "Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WBtFK1hO8KsO"
      },
      "outputs": [],
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "umB5tswsfTEQ"
      },
      "outputs": [],
      "source": [
        "def build_dataset(subset):\n",
        "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_dir,\n",
        "      validation_split=.20,\n",
        "      subset=subset,\n",
        "      label_mode=\"categorical\",\n",
        "      # Seed needs to provided when using validation_split and shuffle = True.\n",
        "      # A fixed seed is used so that the validation set is stable across runs.\n",
        "      seed=123,\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=1)\n",
        "\n",
        "train_ds = build_dataset(\"training\")\n",
        "class_names = tuple(train_ds.class_names)\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "train_ds = train_ds.repeat()\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomRotation(40))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
        "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
        "  # If all training inputs are larger than image_size, one could also use\n",
        "  # RandomCrop with a batch size of 1 and rebatch later.\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "val_ds = build_dataset(\"validation\")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\n",
        "\n",
        "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "50FYNIb1dmJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9cb96b3f-e9b9-4d7e-94e8-554f920894c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\n",
            "Class names: ('flower_photos',)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m1,281\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "hub_layer = hub.KerasLayer(model_handle, trainable=do_fine_tuning)\n",
        "lambda_layer = tf.keras.layers.Lambda(lambda x: hub_layer(x))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    lambda_layer,\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "w_YKX2Qnfg6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "27f131a7-3880-4760-e15a-df3d58b2e3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1), output.shape=(None, 512, 512, 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c0860bd45bba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m hist = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    651\u001b[0m         )\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;34m\"Arguments `target` and `output` must have the same rank \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;34m\"(ndim). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None, 1), output.shape=(None, 512, 512, 2)"
          ]
        }
      ],
      "source": [
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CYOw0fTO1W4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "f020afbb-39aa-4bb7-e600-f4a0b72e1d89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hist' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-ff120fd86ebe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ8DKKgeKv4-"
      },
      "source": [
        "Try out the model on an image from the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oi1iCNB9K1Ai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "42d0cf7f-3823-4c13-844b-19b7a25bdb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACAVJREFUeJzt3c9rZWcBxvHnTlNaq4Pgwn/CteCiO/9I/wFRXLgRQaQ7XbjQtlAXSt1ooYuh0hlJf9BMc18XkzwOpZPcxHvve3LP5wOXLCYDDwPJN+85uWc2Y4wRAEjyaPYAAJZDFAAoUQCgRAGAEgUAShQAKFEAoEQBgBKFnfw+yZ9njwA4OFG41e+SfJDk8ewhAAe38ZiL2/w7yTbJD5KcTd4CcFiiAEC5fARAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCicKsvk3w4ewTAUXhH863G1Us/gdPnYT632ly9AE6fH38BKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgVOznlePMIQuDtR4OT8Osnl7BHwQHl0NifnSZIfxrNt4T5EAYBy+QiAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgWAk3Se5D93/luiAEBtxhhj9ggAlsFJAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRYOHG1Qs4BlFg4S6T/GX2CFgNT0kFoJwUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAqIVF4eskF7NHAKzWwqLwSZI/zB4BsFoLe/bR9UnhrdlDAFZpYVEAYKaFXT4Cjm8keT57BAshCrB6T5L8bPaII/s8yVezRyySKMDqPU7y9uwRR/Y8L+5h8k3uKcDqjbz4Bvn67CEsgJMCrN35L5I//Wb2ChZCFGDt3v0gefZ09goWwuUjWLuLr5LXHiWvuXyEKADwEpePAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgHrgUfg0yWezRwCcjM0YY8wecX/X0zdTVwCcirPZA/4/YgCwTw/88hEA+yQKAJQoAFCiAECJAgAlCgCUKACcvG3+976um4kCwMn7a5KPd/pMUQA4eY+TvLHTZz7wx1wAsE9OCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAKzMyK5PDF0jUQBWZiS5mD1isTwQD4ByUgCgRAGA2j0K7s0AnLzdo/CvJO8dbggA87nRDEC5pwCwCh/t9FlOCgCrsM0u5wAnBYBV2O3bvSgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCjAXn2RZDt7BNybKMBefSfJ5oY/H0meH2kL3N2Co3CZ5Hz2CLijTW6OQiIKLNlmjDFmj/h2Iy/CcDZ7COzR9ZfbbeGAORYcBQCObcGXjwA4NlEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgBIFAEoUAChRgDX7Isnl7BEsyWaMMWaPACa5/urfTF3BgpzNHgBMJAZ8g8tHAJQowGqdJ/l09ggWRhTgoLZXryX6JMlHs0ewMKIAB/X3JO/PHvEKZ0lenz2ChfHbRwCUkwIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKsFcjyWezR8C9iQLs1Ujy4ewRcG/+j2bYq3H18vMWD9PZ7AFwWjZXL3iY/DgDQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCtONJNvZIwCSiMICfJzkb7NHACQRhQV4kuQfs0cAJEk2Y4wxe8S6XVy9vptkM3kLsHZOCtM9TfKrJM8m7wAQhQV4M8mPrj4CzOXyEQDlpABAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoACsxklxefeRVRAFYiZHkn0kuJu9Yts0YQzYBSOKkAMBLRAGAEgUAShQAKFEAoEQBgBIFAEoUgIlGvMN4WUQBmGib5MnsEbzEO5oBKCcF4Mh+nuTL2SN4BVEAjuzN2QO4gctHwIFdJnknybtJfprkJ3PncCNRAA5sJHmW5PMk30/yeOoabiYKAJR7CgCUKABQogBAicLOLpP8dvYIgINyo3ln1/9Mm6krAA7pbPaAh0MMgNPn8hEAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAgAlCgCUKABQogBAiQIAJQoAlCgAUKIAQIkCACUKAJQoAFCiAECJAnAP2yQXs0dwAKIA3NOYPYADuEMUfpnkj0meHmoL8GA8SvLG7BEcwNnun/rjJN9L8tahtgAw2WaM4QwIQBL3FAB4yR2isE1yebAhAMx3hyh8Hb+CBnDa3FMAoNxTAKBEAYASBQBKFAAoUQCgRAGAEgUAShQAKFEAoEQBgPovg+YDqBBf4X0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505ms/step\n",
            "True label: flower_photos\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2eb971e44f53>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpredicted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True label: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted label: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "x, y = next(iter(val_ds))\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "Finally, the trained model can be saved for deployment to TF Serving or TFLite (on mobile) as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LGvTi69oIc2d"
      },
      "outputs": [],
      "source": [
        "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## Optional: Deployment to TensorFlow Lite\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite) lets you deploy TensorFlow models to mobile and IoT devices. The code below shows how to convert the trained model to TFLite and apply post-training tools from the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). Finally, it runs it in the TFLite Interpreter to examine the resulting quality\n",
        "\n",
        "  * Converting without optimization provides the same results as before (up to roundoff error).\n",
        "  * Converting with optimization without any data quantizes the model weights to 8 bits, but inference still uses floating-point computation for the neural network activations. This reduces model size almost by a factor of 4 and improves CPU latency on mobile devices.\n",
        "  * On top, computation of the neural network activations can be quantized to 8-bit integers as well if a small reference dataset is provided to calibrate the quantization range. On a mobile device, this accelerates inference further and makes it possible to run on accelerators like Edge TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Va1Vo92fSyV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773806ba-43ce-4bf2-8230-f07267dddc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote TFLite model of 1788 bytes.\n"
          ]
        }
      ],
      "source": [
        "#@title Optimization settings\n",
        "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
        "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
        "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_ds for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(f\"/tmp/lite_flowers_model_{model_name}.tflite\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_wqEmD0xIqeG"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "JMMK-fZrKrk8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6f46e1b1-5eb1-4df7-cb51-36990b6f2deb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "tensorflow/lite/kernels/read_variable.cc:67 variable != nullptr was not true.Node number 2 (READ_VARIABLE) failed to invoke.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a150c9856de6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcount_lite_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprobs_lite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprobs_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0my_lite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_lite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-bcace3009afc>\u001b[0m in \u001b[0;36mlite_model\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \"\"\"\n\u001b[1;32m    964\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/read_variable.cc:67 variable != nullptr was not true.Node number 2 (READ_VARIABLE) failed to invoke."
          ]
        }
      ],
      "source": [
        "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
        "num_eval_examples = 92  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_ds\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TFLite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TFLite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ],
      "name": "tf2_image_retraining.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}